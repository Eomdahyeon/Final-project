{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df0c299-11ad-4517-ad48-7d9893a97179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê¸°ë³¸\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ê²½ê³  ëœ¨ì§€ ì•Šê²Œ ì„¤ì •\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b066fdf-ee66-4c0e-b7ab-be9f1d9310ad",
   "metadata": {},
   "source": [
    "## ê¸°ë³¸ root path ì„¤ì • (local ì¸ì§€ colabì¸ì§€ í™•ì¸)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c21f600-dca3-4f21-963f-a6006115a4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "### local\n",
    "root_path = '../data/open'\n",
    "\n",
    "### colab\n",
    "# root_path = '/content/drive/MyDrive/12á„Œá…© á„‘á…¡á„‹á…µá„‚á…¥á†¯á„‘á…³á„…á…©á„Œá…¦á†¨á„á…³/data'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0ead64-5cda-4523-a8cc-0e55357f51fd",
   "metadata": {},
   "source": [
    "## ì›”ë³„ ë°ì´í„° ê¸°ë³¸ì „ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164c834d-3b69-472b-8df7-02d832caed09",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ê¸°ë³¸ í´ë” êµ¬ì¡° ì„¤ì •\n",
    "data_splits = [\"train\", \"test\"]\n",
    "\n",
    "# ê° ë°ì´í„° ìœ í˜•ë³„ í´ë”ëª…, íŒŒì¼ ì ‘ë¯¸ì‚¬, ë³€ìˆ˜ ì ‘ë‘ì–´ ì„¤ì •\n",
    "data_categories = {\n",
    "    #\"íšŒì›ì •ë³´\": {\"folder\": \"1.íšŒì›ì •ë³´\", \"suffix\": \"íšŒì›ì •ë³´\", \"var_prefix\": \"customer\"},\n",
    "    #\"ì‹ ìš©ì •ë³´\": {\"folder\": \"2.ì‹ ìš©ì •ë³´\", \"suffix\": \"ì‹ ìš©ì •ë³´\", \"var_prefix\": \"credit\"},\n",
    "    \"ìŠ¹ì¸ë§¤ì¶œì •ë³´\": {\"folder\": \"3.ìŠ¹ì¸ë§¤ì¶œì •ë³´\", \"suffix\": \"ìŠ¹ì¸ë§¤ì¶œì •ë³´\", \"var_prefix\": \"sales\"},\n",
    "    #\"ì²­êµ¬ì •ë³´\": {\"folder\": \"4.ì²­êµ¬ì…ê¸ˆì •ë³´\", \"suffix\": \"ì²­êµ¬ì •ë³´\", \"var_prefix\": \"billing\"},\n",
    "    #\"ì”ì•¡ì •ë³´\": {\"folder\": \"5.ì”ì•¡ì •ë³´\", \"suffix\": \"ì”ì•¡ì •ë³´\", \"var_prefix\": \"balance\"},\n",
    "    #\"ì±„ë„ì •ë³´\": {\"folder\": \"6.ì±„ë„ì •ë³´\", \"suffix\": \"ì±„ë„ì •ë³´\", \"var_prefix\": \"channel\"},\n",
    "    #\"ë§ˆì¼€íŒ…ì •ë³´\": {\"folder\": \"7.ë§ˆì¼€íŒ…ì •ë³´\", \"suffix\": \"ë§ˆì¼€íŒ…ì •ë³´\", \"var_prefix\": \"marketing\"},\n",
    "    #\"ì„±ê³¼ì •ë³´\": {\"folder\": \"8.ì„±ê³¼ì •ë³´\", \"suffix\": \"ì„±ê³¼ì •ë³´\", \"var_prefix\": \"performance\"}\n",
    "}\n",
    "\n",
    "months = [\"07\",\"08\",\"09\",\"10\",\"11\",\"12\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8d9dbe-65d6-4971-80cc-6e73984dc692",
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in data_splits:\n",
    "    for category, info in data_categories.items():\n",
    "        folder = info[\"folder\"]\n",
    "        suffix = info[\"suffix\"]\n",
    "        var_prefix = info[\"var_prefix\"]\n",
    "\n",
    "        for month in months:\n",
    "            if split == \"test\":\n",
    "                if month != \"12\":\n",
    "                    continue\n",
    "                    \n",
    "            ### íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "            # íŒŒì¼ëª… í˜•ì‹: 2018{month}_{split}_{suffix}.parquet\n",
    "            file_path = f\"{root_path}/{split}/{folder}/2018{month}_{split}_{suffix}.parquet\"\n",
    "            df = pd.read_parquet(file_path)\n",
    "\n",
    "            ### Label Encoding\n",
    "            # ë²”ì£¼í˜• -> ìˆ˜ì¹˜í˜•\n",
    "            if category == \"íšŒì›ì •ë³´\":\n",
    "                # '_1ìˆœìœ„ì‹ ìš©ì²´í¬êµ¬ë¶„', '_2ìˆœìœ„ì‹ ìš©ì²´í¬êµ¬ë¶„' â†’ ê²°ì¸¡ì¹˜ 'ê¸°íƒ€'ë¡œ ì±„ìš°ê¸° ë° ì¸ì½”ë”©\n",
    "                df['_1ìˆœìœ„ì‹ ìš©ì²´í¬êµ¬ë¶„'] = df['_1ìˆœìœ„ì‹ ìš©ì²´í¬êµ¬ë¶„'].fillna('ê¸°íƒ€')\n",
    "                df['_2ìˆœìœ„ì‹ ìš©ì²´í¬êµ¬ë¶„'] = df['_2ìˆœìœ„ì‹ ìš©ì²´í¬êµ¬ë¶„'].fillna('ê¸°íƒ€')\n",
    "                mapping = {'ì‹ ìš©': 1, 'ì²´í¬': 0, 'ê¸°íƒ€': -1}\n",
    "                df['1ìˆœìœ„ì‹ ìš©ì²´í¬êµ¬ë¶„_ì¸ì½”ë”©'] = df['_1ìˆœìœ„ì‹ ìš©ì²´í¬êµ¬ë¶„'].map(mapping)\n",
    "                df['2ìˆœìœ„ì‹ ìš©ì²´í¬êµ¬ë¶„_ì¸ì½”ë”©'] = df['_2ìˆœìœ„ì‹ ìš©ì²´í¬êµ¬ë¶„'].map(mapping)\n",
    "                df.drop(columns=['_1ìˆœìœ„ì‹ ìš©ì²´í¬êµ¬ë¶„','_2ìˆœìœ„ì‹ ìš©ì²´í¬êµ¬ë¶„'], inplace=True)\n",
    "\n",
    "                # ê°€ì…í†µì‹ íšŒì‚¬ì½”ë“œ â†’ ê°€ì…í†µì‹ íšŒì‚¬_Sì‚¬ì—¬ë¶€\n",
    "                df['ê°€ì…í†µì‹ íšŒì‚¬_Sì‚¬ì—¬ë¶€'] = (df['ê°€ì…í†µì‹ íšŒì‚¬ì½”ë“œ'] == 'Sì‚¬').astype(int)\n",
    "                df.drop(columns=['ê°€ì…í†µì‹ íšŒì‚¬ì½”ë“œ'], inplace=True)\n",
    "\n",
    "                # ì§ì¥ì‹œë„ëª… â†’ ì§ì¥_ìˆ˜ë„ê¶Œì—¬ë¶€\n",
    "                df['ì§ì¥_ìˆ˜ë„ê¶Œì—¬ë¶€'] = df['ì§ì¥ì‹œë„ëª…'].isin(['ì„œìš¸', 'ê²½ê¸°']).astype(int)\n",
    "                df.drop(columns=['ì§ì¥ì‹œë„ëª…'], inplace=True)\n",
    "\n",
    "                # ê²°ì¸¡ì¹˜ ë§ì€ ë³€ìˆ˜ ì‚­ì œ\n",
    "                df.drop(columns=['ìµœì¢…ì¹´ë“œë°œê¸‰ì¼ì','ìµœì¢…ìœ íš¨ë…„ì›”_ì‹ ìš©_ì´ìš©ê°€ëŠ¥','ìµœì¢…ìœ íš¨ë…„ì›”_ì‹ ìš©_ì´ìš©'], inplace=True)\n",
    "\n",
    "                ##### 2. ìë£Œí˜• ë³€í™˜ ë° íŒŒìƒë³€ìˆ˜ ìƒì„±\n",
    "\n",
    "                # ê±°ì£¼ì‹œë„ëª… â†’ ê±°ì£¼ì§€_ìˆ˜ë„ê¶Œì—¬ë¶€\n",
    "                df['ê±°ì£¼ì§€_ìˆ˜ë„ê¶Œì—¬ë¶€'] = df['ê±°ì£¼ì‹œë„ëª…'].isin(['ì„œìš¸', 'ê²½ê¸°']).astype(int)\n",
    "                df.drop(columns=['ê±°ì£¼ì‹œë„ëª…'], inplace=True)\n",
    "\n",
    "                # ì—°íšŒë¹„ë°œìƒì¹´ë“œìˆ˜_B0M â†’ ì´ì§„ë³€ìˆ˜í™”\n",
    "                df['ì—°íšŒë¹„ë°œìƒì¹´ë“œìˆ˜_B0M_ì´ì§„'] = df['ì—°íšŒë¹„ë°œìƒì¹´ë“œìˆ˜_B0M'].isin(['1ê°œì´ìƒ']).astype(int)\n",
    "                df.drop(columns=['ì—°íšŒë¹„ë°œìƒì¹´ë“œìˆ˜_B0M'], inplace=True)\n",
    "\n",
    "                # ë°ì´í„°ê°€ ëª¨ë‘ ë™ì¼í•œ ë³€ìˆ˜ ì œê±°\n",
    "                columns_drop = ['ìƒí’ˆê´€ë ¨ë©´ì œì¹´ë“œìˆ˜_B0M','ì„ì§ì›ë©´ì œì¹´ë“œìˆ˜_B0M','ìš°ìˆ˜íšŒì›ë©´ì œì¹´ë“œìˆ˜_B0M','ê¸°íƒ€ë©´ì œì¹´ë“œìˆ˜_B0M']\n",
    "                df.drop(columns=columns_drop, inplace=True)\n",
    "\n",
    "                # Life_Stage â†’ íŒŒìƒë³€ìˆ˜ ìƒì„± í›„ ì‚­ì œ\n",
    "                df['Life_Stage_ìë…€ì„±ì¥_ì—¬ë¶€'] = df['Life_Stage'].isin(['ìë…€ì„±ì¥(1)', 'ìë…€ì„±ì¥(2)']).astype(int)\n",
    "                df.drop(columns=['Life_Stage'], inplace=True)\n",
    "\n",
    "                # ì—°ë ¹: ìˆ«ìë§Œ ì¶”ì¶œ â†’ intí˜• ë³€í™˜\n",
    "                df['ì—°ë ¹'] = df['ì—°ë ¹'].str.extract(r'(\\d+)').astype(float).astype('Int64')\n",
    "\n",
    "                ##### 3. ìƒê´€ê´€ê³„ ê³ ë ¤ ë³€ìˆ˜ ì œê±°\n",
    "                columns_drop = ['ì´ìš©ê¸ˆì•¡_R3M_ì²´í¬_ê°€ì¡±','ì—°íšŒë¹„í• ì¸ì¹´ë“œìˆ˜_B0M','í• ì¸ê¸ˆì•¡_ê¸°ë³¸ì—°íšŒë¹„_B0M','í• ì¸ê¸ˆì•¡_ì œíœ´ì—°íšŒë¹„_B0M']\n",
    "                df.drop(columns=columns_drop, inplace=True)\n",
    "\n",
    "                ##### 4. ìœ ì‚¬/ì¤‘ë³µ ì˜ë¯¸ ë³€ìˆ˜ ì œê±°\n",
    "                columns_drop = ['ì…íšŒì¼ì_ì‹ ìš©','ì´ìš©ì¹´ë“œìˆ˜_ì²´í¬_ê°€ì¡±','ì²­êµ¬ê¸ˆì•¡_ê¸°ë³¸ì—°íšŒë¹„_B0M','ì²­êµ¬ê¸ˆì•¡_ì œíœ´ì—°íšŒë¹„_B0M']\n",
    "                df.drop(columns=columns_drop, inplace=True)\n",
    "                \n",
    "            if category == \"ì‹ ìš©ì •ë³´\":\n",
    "                # 'RVì „í™˜ê°€ëŠ¥ì—¬ë¶€' â†’ 'RVì „í™˜ë¶ˆê°€ëŠ¥ì—¬ë¶€'\n",
    "                df['RV_ì „í™˜ê°€ëŠ¥ì—¬ë¶€_ì´ì§„'] = (df['RVì „í™˜ê°€ëŠ¥ì—¬ë¶€'] == 'N').astype(int)\n",
    "                df = df.drop(columns='RVì „í™˜ê°€ëŠ¥ì—¬ë¶€')\n",
    "\n",
    "                ##### 2. ìë£Œí˜• ë³€í™˜\n",
    "                # ìë°œí•œë„ê°ì•¡íšŸìˆ˜_R12M: '0íšŒ' â†’ 0, '1íšŒ' â†’ 1, ...\n",
    "                df['ìë°œí•œë„ê°ì•¡íšŸìˆ˜_R12M'] = (\n",
    "                    df['ìë°œí•œë„ê°ì•¡íšŸìˆ˜_R12M']\n",
    "                    .str.replace('íšŒ', '', regex=False)\n",
    "                    .astype(int)\n",
    "                )\n",
    "\n",
    "                # â€˜í•œë„ì¦ì•¡íšŸìˆ˜_R12Mâ€™ â†’ 'í•œë„ì¦ì•¡_R12M_ì—¬ë¶€': '0íšŒ' â†’ 0, '1íšŒì´ìƒ' â†’ 1\n",
    "                df['í•œë„ì¦ì•¡_R12M_ì—¬ë¶€'] = df['í•œë„ì¦ì•¡íšŸìˆ˜_R12M'].map({\n",
    "                    '0íšŒ': 0,\n",
    "                    '1íšŒì´ìƒ': 1\n",
    "                }).astype(int)\n",
    "                df.drop(columns=['í•œë„ì¦ì•¡íšŸìˆ˜_R12M'], inplace=True)\n",
    "\n",
    "                # 'ì¹´ë“œë¡ ë™ì˜ì—¬ë¶€': 'Y' â†’ 1, 'N' â†’ 0\n",
    "                df['ì¹´ë“œë¡ ë™ì˜ì—¬ë¶€'] = df['ì¹´ë“œë¡ ë™ì˜ì—¬ë¶€'].map({'Y': 1, 'N': 0}).astype(int)\n",
    "\n",
    "                # 'í•œë„ì‹¬ì‚¬ìš”ì²­ê±´ìˆ˜' â†’ 'í•œë„ì‹¬ì‚¬ìš”ì²­ì—¬ë¶€': '0íšŒ' â†’ 0, '1íšŒì´ìƒ' â†’ 1\n",
    "                df['í•œë„ì‹¬ì‚¬ìš”ì²­ì—¬ë¶€'] = df['í•œë„ì‹¬ì‚¬ìš”ì²­ê±´ìˆ˜'].map({\n",
    "                    '0íšŒ': 0,\n",
    "                    '1íšŒì´ìƒ': 1\n",
    "                }).astype(int)\n",
    "                df.drop(columns=['í•œë„ì‹¬ì‚¬ìš”ì²­ê±´ìˆ˜'], inplace=True)\n",
    "                \n",
    "                ##### 4. ë³€ìˆ˜ ë³„ í™•ì¸ ë° íŒŒìƒ ë³€ìˆ˜ ìƒì„±\n",
    "                df['RVì‹¤ì‚¬ìš©ì—¬ë¶€'] = (df['RVì•½ì •ì²­êµ¬ìœ¨'] > 0).astype(int)\n",
    "                df['ê°•ì œí•œë„ê°ì•¡íšŸìˆ˜_2íšŒì´ìƒì—¬ë¶€'] = (df['ê°•ì œí•œë„ê°ì•¡íšŸìˆ˜_R12M'] > 1).astype(int)\n",
    "                df['ê°•ì œí•œë„ê°ì•¡ê¸ˆì•¡_R12M_3ì´ìƒì—¬ë¶€'] = (df['ê°•ì œí•œë„ê°ì•¡ê¸ˆì•¡_R12M'] > 2).astype(int)\n",
    "                df['ìƒí–¥ê°€ëŠ¥CAí•œë„ê¸ˆì•¡_1ì—¬ë¶€'] = (df['ìƒí–¥ê°€ëŠ¥CAí•œë„ê¸ˆì•¡'] == 1).astype(int)\n",
    "                \n",
    "            if category == \"ìŠ¹ì¸ë§¤ì¶œì •ë³´\":\n",
    "                df['ì´ìš©ê¸ˆì•¡ëŒ€'] = df['ì´ìš©ê¸ˆì•¡ëŒ€'].map({\n",
    "                    '09.ë¯¸ì‚¬ìš©' : 0,\n",
    "                    '05.10ë§Œì›-' : 1,\n",
    "                    '04.10ë§Œì›+' : 2,\n",
    "                    '03.30ë§Œì›+' : 3,\n",
    "                    '02.50ë§Œì›+' : 4,\n",
    "                    '01.100ë§Œì›+' : 5})\n",
    "                print(\"ğŸ”€ ìˆ˜ì¹˜í˜•ë³€ìˆ˜ë¡œ ì¸ì½”ë”©\")\n",
    "\n",
    "            if category == \"ì²­êµ¬ì •ë³´\":\n",
    "                df['ëŒ€í‘œì²­êµ¬ì§€ê³ ê°ì£¼ì†Œêµ¬ë¶„ì½”ë“œ'] = df['ëŒ€í‘œì²­êµ¬ì§€ê³ ê°ì£¼ì†Œêµ¬ë¶„ì½”ë“œ'].map({\n",
    "                    'ë¯¸í™•ì¸':0, 'ì£¼ê±°ì§€':1, 'íšŒì‚¬':2})\n",
    "                df['ëŒ€í‘œì²­êµ¬ì„œìˆ˜ë ¹ì§€êµ¬ë¶„ì½”ë“œ'] = df['ëŒ€í‘œì²­êµ¬ì„œìˆ˜ë ¹ì§€êµ¬ë¶„ì½”ë“œ'].map({\n",
    "                    'ìš°í¸':0, 'ì´ë©”ì¼':1, 'ë‹¹ì‚¬í˜ì´ì•±+ì´ë©”ì¼':2,'Kí†¡ëª…ì„¸ì„œ+ì´ë©”ì¼':3, 'ë¯¸ìˆ˜ì‹ ':4,\n",
    "                    'ë‹¹ì‚¬ë©¤ë²„ì‹­+ì´ë©”ì¼':5, 'ë¬¸ìë©”ì„¸ì§€':6})\n",
    "                df['ì²­êµ¬ì„œìˆ˜ë ¹ë°©ë²•'] = df['ì²­êµ¬ì„œìˆ˜ë ¹ë°©ë²•'].map({\n",
    "                    'ìš°í¸':0, 'ì´ë©”ì¼':1, 'ë¬¸ìë©”ì„¸ì§€':2,\n",
    "                    'Kí†¡':3, 'ë¯¸ìˆ˜ë ¹':4, 'ë‹¹ì‚¬ë©¤ë²„ì‹­':5})\n",
    "                                                  \n",
    "            if category == \"ì±„ë„ì •ë³´\":\n",
    "                df['ì¸ì…íšŸìˆ˜_ARS_R6M'] = df['ì¸ì…íšŸìˆ˜_ARS_R6M'].map({\n",
    "                    '1íšŒ ì´ìƒ': 0, '10íšŒ ì´ìƒ': 1})\n",
    "                df['ì´ìš©ë©”ë‰´ê±´ìˆ˜_ARS_R6M'] = df['ì´ìš©ë©”ë‰´ê±´ìˆ˜_ARS_R6M'].map({\n",
    "                    '1íšŒ ì´ìƒ': 0, '10íšŒ ì´ìƒ': 1, '20íšŒ ì´ìƒ': 2, '30íšŒ ì´ìƒ': 3})\n",
    "                df['ë°©ë¬¸íšŸìˆ˜_PC_R6M'] = df['ë°©ë¬¸íšŸìˆ˜_PC_R6M'].map({\n",
    "                    '1íšŒ ì´ìƒ': 0, '10íšŒ ì´ìƒ': 1, '20íšŒ ì´ìƒ': 2, '30íšŒ ì´ìƒ': 3, '40íšŒ ì´ìƒ': 4})\n",
    "                df['ë°©ë¬¸ì¼ìˆ˜_PC_R6M'] = df['ë°©ë¬¸ì¼ìˆ˜_PC_R6M'].map({\n",
    "                    '1íšŒ ì´ìƒ': 0, '10íšŒ ì´ìƒ': 1, '20íšŒ ì´ìƒ': 2, '30íšŒ ì´ìƒ': 3})\n",
    "                df['ë°©ë¬¸íšŸìˆ˜_ì•±_R6M'] = df['ë°©ë¬¸íšŸìˆ˜_ì•±_R6M'].map({\n",
    "                    '1íšŒ ì´ìƒ': 0, '10íšŒ ì´ìƒ': 1, '20íšŒ ì´ìƒ': 2, '30íšŒ ì´ìƒ': 3, '40íšŒ ì´ìƒ': 4, '50íšŒ ì´ìƒ': 5, '60íšŒ ì´ìƒ': 6, '70íšŒ ì´ìƒ': 7, '80íšŒ ì´ìƒ': 8})\n",
    "\n",
    "            if category == \"ë§ˆì¼€íŒ…ì •ë³´\":\n",
    "                df['ìº í˜ì¸ì ‘ì´‰ê±´ìˆ˜_R12M'] = df['ìº í˜ì¸ì ‘ì´‰ê±´ìˆ˜_R12M'].map({\n",
    "                    '1íšŒ ì´ìƒ': 0, '5íšŒ ì´ìƒ': 1, '10íšŒ ì´ìƒ': 2,'15íšŒ ì´ìƒ': 3, '20íšŒ ì´ìƒ': 4, '25íšŒ ì´ìƒ': 5})\n",
    "                df['ìº í˜ì¸ì ‘ì´‰ì¼ìˆ˜_R12M'] = df['ìº í˜ì¸ì ‘ì´‰ì¼ìˆ˜_R12M'].map({'1ì¼ ì´ìƒ': 0, '5ì¼ ì´ìƒ': 1, '10ì¼ ì´ìƒ': 2, '15ì¼ ì´ìƒ': 3, '20ì¼ ì´ìƒ': 4})\n",
    "                \n",
    "            ### ì»¬ëŸ¼ ì œê±°\n",
    "            # ëª¨ë“  ê°’ì´ ë™ì¼í•œ ì»¬ëŸ¼ ì œê±° (ê¸°ì¤€ë…„ë„ ì»¬ëŸ¼ ì œì™¸)\n",
    "            constant_cols = [col for col in df.columns.difference(['ê¸°ì¤€ë…„ì›”']) if df[col].nunique() == 1]\n",
    "            if constant_cols:\n",
    "                print(f\"ğŸ§¹ ë™ì¼ê°’ ì»¬ëŸ¼ ì œê±°: {constant_cols}\")\n",
    "                df = df.drop(columns=constant_cols)\n",
    "            \n",
    "            # ê²°ì¸¡ì¹˜ê°€ ìˆëŠ” ì»¬ëŸ¼ ì œê±°\n",
    "            null_cols = df.columns[df.isnull().any()]\n",
    "            if len(null_cols) > 0:\n",
    "                print(f\"ğŸ§¹ ê²°ì¸¡ì¹˜ ì»¬ëŸ¼ ì œê±°: {list(null_cols)}\")\n",
    "                df = df.drop(columns=null_cols)\n",
    "            \n",
    "            ### One-hot Encoding\n",
    "            # segment ê°’ ìˆëŠ” íŒŒì¼ ì£¼ì†Œ ì„¤ì •\n",
    "            segment_folder = f'{root_path}/{split}/1.íšŒì›ì •ë³´'\n",
    "            segment_category = \"íšŒì›ì •ë³´\"\n",
    "            \n",
    "            # ì„¸ê·¸ë¨¼íŠ¸ ì»¬ëŸ¼ ì¶”ê°€\n",
    "            if split==\"train\":\n",
    "                if category!=\"íšŒì›ì •ë³´\":\n",
    "                    segment_df = pd.read_parquet(f\"{segment_folder}/2018{month}_{split}_{segment_category}.parquet\")\n",
    "                    print(\"ğŸ”€ ì„¸ê·¸ë¨¼íŠ¸ ë³‘í•©\")\n",
    "                    df = df.merge(segment_df[['ID', 'Segment']], on='ID', how='left')\n",
    "                if category == \"ì‹ ìš©ì •ë³´\":\n",
    "                    # ì¹´ë“œì´ìš©í•œë„ê¸ˆì•¡_Aìˆ˜ì¤€ë³µí•© íŒŒìƒë³€ìˆ˜ ìƒì„±\n",
    "                    a_limit = df[df['Segment'] == 'A']['ì¹´ë“œì´ìš©í•œë„ê¸ˆì•¡_B1M']\n",
    "                    q1, q3 = a_limit.quantile(0.25), a_limit.quantile(0.75)\n",
    "                    iqr = q3 - q1\n",
    "                    lower, upper = q1 - 1.5 * iqr, q3 + 1.5 * iqr\n",
    "                    a_B1M = a_limit[(a_limit >= lower) & (a_limit <= upper)].min()\n",
    "\n",
    "                    a_limit = df[df['Segment'] == 'A']['ì¹´ë“œì´ìš©í•œë„ê¸ˆì•¡_B2M']\n",
    "                    q1, q3 = a_limit.quantile(0.25), a_limit.quantile(0.75)\n",
    "                    iqr = q3 - q1\n",
    "                    lower, upper = q1 - 1.5 * iqr, q3 + 1.5 * iqr\n",
    "                    a_B2M = a_limit[(a_limit >= lower) & (a_limit <= upper)].min()\n",
    "\n",
    "                    def classify_dual_limit(row):\n",
    "                        b1 = row['ì¹´ë“œì´ìš©í•œë„ê¸ˆì•¡_B1M'] >= a_B1M\n",
    "                        b2 = row['ì¹´ë“œì´ìš©í•œë„ê¸ˆì•¡_B2M'] >= a_B2M\n",
    "                        return 2 if b1 and b2 else 1 if b1 or b2 else 0\n",
    "\n",
    "                    df['ì¹´ë“œì´ìš©í•œë„ê¸ˆì•¡_Aìˆ˜ì¤€ë³µí•©'] = df.apply(classify_dual_limit, axis=1)\n",
    "                \n",
    "                df=pd.get_dummies(df, columns=['Segment'])\n",
    "            \n",
    "            # ë©”ëª¨ë¦¬ ìµœì í™”\n",
    "            for col in df.select_dtypes(include='int64').columns:\n",
    "                if df[col].max() < 2_147_483_647:\n",
    "                    df[col] = df[col].astype('int32')\n",
    "\n",
    "            for col in df.select_dtypes(include='float64').columns:\n",
    "                df[col] = df[col].astype('float32')\n",
    "\n",
    "            ### ì „ì²˜ë¦¬ëœ íŒŒì¼ ì €ì¥\n",
    "            output_file=f\"{root_path}/{split}/{folder}/2018{month}_processed_{category}.parquet\"\n",
    "            df.to_parquet(output_file, index=False)\n",
    "            print(f\"âœ… ì €ì¥ ì™„ë£Œ: {month}_{split}_{category}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c6c1cb-3eb0-4261-90b5-5532d7cbc7a2",
   "metadata": {},
   "source": [
    "## ì „ì²˜ë¦¬ëœ ì›”ë³„ ë°ì´í„° í•˜ë‚˜ë¡œ í†µí•©"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b4f255-f8fd-4971-898d-6b31069bec90",
   "metadata": {},
   "source": [
    "#### ì‹¤í–‰ì‹œ ë©”ëª¨ë¦¬ ì—ëŸ¬ ëœ¨ëŠ” ê²½ìš°ì—ëŠ” restart kernel í•˜ì‹  í›„ ê¸°ë³¸í´ë”êµ¬ì¡°ê¹Œì§€ë§Œ ì‹¤í–‰í›„ ë°”ë¡œ ì•„ë˜ ì½”ë“œ ì‹¤í–‰í•´ë³´ì„¸ìš”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dff8483-53b2-4f6c-96d1-84cf4d5a2128",
   "metadata": {},
   "outputs": [],
   "source": [
    "split = \"train\"\n",
    "merged_list=[]\n",
    "for category, info in data_categories.items():\n",
    "    folder = info[\"folder\"]\n",
    "    suffix = info[\"suffix\"]\n",
    "    var_prefix = info[\"var_prefix\"]\n",
    "\n",
    "    for month in months:\n",
    "        ### íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "        file_path = f\"{root_path}/{split}/{folder}/2018{month}_processed_{category}.parquet\"\n",
    "        df = pd.read_parquet(file_path)\n",
    "        merged_list.append(df)\n",
    "        print(f\"âœ… {file_path} ë³€í™˜ ì™„ë£Œ\")\n",
    "\n",
    "    # íŒŒì¼ ì €ì¥\n",
    "    if merged_list:\n",
    "        merged_df = pd.concat(merged_list, ignore_index=True)\n",
    "        \n",
    "        # ê²°ì¸¡ì¹˜ê°€ ìˆëŠ” ì»¬ëŸ¼ ì œê±°\n",
    "        null_cols = merged_df.columns[merged_df.isnull().any()]\n",
    "        if len(null_cols) > 0:\n",
    "            print(f\"ğŸ§¹ ê²°ì¸¡ì¹˜ ì»¬ëŸ¼ ì œê±°: {list(null_cols)}\")\n",
    "            merged_df = merged_df.drop(columns=null_cols)\n",
    "        \n",
    "        output_file = f\"{root_path}/{split}/{folder}/cleaned_{suffix}.parquet\"\n",
    "        merged_df.to_parquet(output_file, index=False)\n",
    "        print(f\"âœ… ì „ì²´ ë³‘í•© ì €ì¥ ì™„ë£Œ: ì €ì¥ ì™„ë£Œ (Shape: {merged_df.shape})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4592588-bf0a-4328-b3a1-82738e7a73b1",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
