{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad80858d",
   "metadata": {},
   "source": [
    "### ë‹¨ì¼ëª¨ë¸ë§ (Segment_A~E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263c212e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# local\n",
    "root_path = '../data/open'\n",
    "\n",
    "# colab\n",
    "# root_path = '/content/drive/MyDrive/12á„Œá…© á„‘á…¡á„‹á…µá„‚á…¥á†¯á„‘á…³á„…á…©á„Œá…¦á†¨á„á…³/data'\n",
    "\n",
    "# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "\n",
    "#train_file = '201812_processed_All'\n",
    "#train_file = '201812_corr_drop_All'\n",
    "#file_name = 'corr_drop_All'\n",
    "#file_name = '201812_vif_drop_All'\n",
    "file_name = '201812_vif_drop_threshold20'\n",
    "train_df = pd.read_parquet(f'{root_path}/train/{file_name}.parquet')\n",
    "\n",
    "test_file = f'{root_path}/test/201812_processed_All.parquet'\n",
    "test_df = pd.read_parquet(test_file)\n",
    "\n",
    "# íƒ€ê²Ÿ ë ˆì´ë¸” ì¬êµ¬ì„±\n",
    "def get_target_label(df):\n",
    "    segment_columns = ['Segment_A', 'Segment_B', 'Segment_C', 'Segment_D', 'Segment_E']\n",
    "    return df[segment_columns].idxmax(axis=1).str[-1]\n",
    "\n",
    "# íƒ€ê²Ÿ ìƒì„±\n",
    "train_df['Segment'] = get_target_label(train_df)\n",
    "\n",
    "# í•™ìŠµìš© í”¼ì²˜/íƒ€ê²Ÿ ì •ì˜\n",
    "X = train_df.drop(columns=['Segment_A', 'Segment_B', 'Segment_C', 'Segment_D', 'Segment_E', 'Segment', 'ID', 'ê¸°ì¤€ë…„ì›”'])\n",
    "y = train_df['Segment']\n",
    "\n",
    "# Xì™€ test_df ì–‘ìª½ì— ëª¨ë‘ ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ ì„ íƒ\n",
    "common_cols = [col for col in X.columns if col in test_df.columns]\n",
    "X = X[common_cols]\n",
    "X_test = test_df[common_cols]\n",
    "\n",
    "# í•™ìŠµ/ê²€ì¦ ë¶„ë¦¬\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# CatBoost ëª¨ë¸ ì •ì˜ ë° í•™ìŠµ\n",
    "'''\n",
    "model = CatBoostClassifier(\n",
    "    iterations=1000,\n",
    "    learning_rate=0.05,\n",
    "    depth=6,\n",
    "    loss_function='MultiClass',\n",
    "    eval_metric='TotalF1:average=Micro',\n",
    "    verbose=100,\n",
    "    random_seed=42\n",
    ")\n",
    "'''\n",
    "model = CatBoostClassifier(verbose=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# ê²€ì¦ ì„±ëŠ¥ í‰ê°€\n",
    "y_pred_val = model.predict(X_val)\n",
    "f1 = f1_score(y_val, y_pred_val, average='micro')\n",
    "print(f\"Validation F1 score (micro): {f1:.4f}\")\n",
    "\n",
    "# ìµœì¢… ì˜ˆì¸¡\n",
    "y_pred_test = model.predict(X_test).flatten()\n",
    "\n",
    "# ì œì¶œíŒŒì¼ ìƒì„±\n",
    "submission = pd.DataFrame({\n",
    "    'ID': test_df['ID'],\n",
    "    'Segment': y_pred_test\n",
    "})\n",
    "\n",
    "submission.to_csv(f'../results/{file_name}_catboost_submission.csv', index=False)\n",
    "print(f\"{file_name}_submission ì €ì¥ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d64129f",
   "metadata": {},
   "source": [
    "### ë‹¨ì¼ ëª¨ë¸ë§ (Segment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cefcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# local\n",
    "root_path = '../data/open'\n",
    "\n",
    "# colab\n",
    "# root_path = '/content/drive/MyDrive/12á„Œá…© á„‘á…¡á„‹á…µá„‚á…¥á†¯á„‘á…³á„…á…©á„Œá…¦á†¨á„á…³/data'\n",
    "\n",
    "# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "file_name = 'vif_one_segment' # ì´ ë¶€ë¶„ë§Œ ìˆ˜ì •í•˜ì—¬ ëª¨ë¸1,2,3 ë˜‘ê°™ì´ ê²€ì •\n",
    "train_df = pd.read_parquet(f'{root_path}/train/{file_name}.parquet')\n",
    "\n",
    "# í•™ìŠµìš© í”¼ì²˜/íƒ€ê²Ÿ ì •ì˜\n",
    "X = train_df.drop(columns=['ID', 'ê¸°ì¤€ë…„ì›”', 'Segment'])\n",
    "y = train_df['Segment']\n",
    "\n",
    "test_file = f'{root_path}/test/201812_processed_All.parquet'\n",
    "test_df = pd.read_parquet(test_file)\n",
    "\n",
    "# Xì™€ test_df ì–‘ìª½ì— ëª¨ë‘ ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ ì„ íƒ\n",
    "common_cols = [col for col in X.columns if col in test_df.columns]\n",
    "X = X[common_cols]\n",
    "X_test = test_df[common_cols]\n",
    "\n",
    "# í•™ìŠµ/ê²€ì¦ ë¶„ë¦¬\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# CatBoost ëª¨ë¸ ì •ì˜ ë° í•™ìŠµ\n",
    "'''\n",
    "model = CatBoostClassifier(\n",
    "    iterations=1000,\n",
    "    learning_rate=0.05,\n",
    "    depth=6,\n",
    "    loss_function='MultiClass',\n",
    "    eval_metric='TotalF1:average=Micro',\n",
    "    verbose=100,\n",
    "    random_seed=42\n",
    ")\n",
    "'''\n",
    "model = CatBoostClassifier(verbose=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# ê²€ì¦ ì„±ëŠ¥ í‰ê°€\n",
    "y_pred_val = model.predict(X_val)\n",
    "f1 = f1_score(y_val, y_pred_val, average='micro')\n",
    "print(f\"Validation F1 score (micro): {f1:.4f}\")\n",
    "\n",
    "# í˜¼ë™í–‰ë ¬\n",
    "# í˜¼ë™í–‰ë ¬ì„ DataFrameìœ¼ë¡œ ë³´ê¸° ì¢‹ê²Œ ì¶œë ¥\n",
    "labels = ['A', 'B', 'C', 'D', 'E']\n",
    "cm = confusion_matrix(y_val, y_pred_val, labels=labels)\n",
    "cm_df = pd.DataFrame(cm, index=[f\"ì‹¤ì œ_{l}\" for l in labels], columns=[f\"ì˜ˆì¸¡_{l}\" for l in labels])\n",
    "\n",
    "print(\"ğŸ“Š Confusion Matrix\")\n",
    "display(cm_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cd1e59",
   "metadata": {},
   "source": [
    "### íŒŒì´í”„ë¼ì¸ ëª¨ë¸ë§ (3ë‹¨ê³„)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2b0afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# local\n",
    "root_path = '../data/open'\n",
    "\n",
    "# colab\n",
    "# root_path = '/content/drive/MyDrive/12á„Œá…© á„‘á…¡á„‹á…µá„‚á…¥á†¯á„‘á…³á„…á…©á„Œá…¦á†¨á„á…³/data'\n",
    "\n",
    "# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "\n",
    "#train_file = '201812_vif_drop_All'\n",
    "#train_file = '201812_corr_drop_All'\n",
    "#train_file = '201812_processed_All'\n",
    "train_file = 'vif_one_segment'\n",
    "#train_file = 'corr_one_segment'\n",
    "origin_df = pd.read_parquet(f'{root_path}/train/{train_file}.parquet')\n",
    "\n",
    "test_file = f'{root_path}/test/201812_processed_All.parquet'\n",
    "test_df = pd.read_parquet(test_file)\n",
    "\n",
    "y_origin = origin_df['Segment']\n",
    "\n",
    "### í•™ìŠµ/ê²€ì¦ ë¶„ë¦¬ (ì¸ë±ìŠ¤ ìœ ì§€/ y_origin í´ë˜ìŠ¤ ë¹„ìœ¨ì— ë§ì¶°ì„œ)\n",
    "train_df, score_df = train_test_split(origin_df, test_size=0.2, random_state=42, stratify=y_origin)\n",
    "\n",
    "print(\"âœ… ë‹¨ê³„ 1: Segment == E vs Other\")\n",
    "\n",
    "# ë¶„ë¥˜ ë³€ê²½\n",
    "train_df[\"Segment1\"] = train_df[\"Segment\"].apply(lambda x: \"E\" if x == \"E\" else \"other\")\n",
    "\n",
    "exclude_cols = ['ID', 'ê¸°ì¤€ë…„ì›”', 'Segment', 'Segment1']\n",
    "feature_cols = [\n",
    "    col for col in train_df.columns\n",
    "    if col not in exclude_cols\n",
    "]\n",
    "\n",
    "# í•™ìŠµìš© í”¼ì²˜/íƒ€ê²Ÿ ì •ì˜\n",
    "X = train_df[feature_cols]\n",
    "\n",
    "# Xì™€ test_df ì–‘ìª½ì— ëª¨ë‘ ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ ì„ íƒ\n",
    "common_cols = [col for col in X.columns if col in test_df.columns]\n",
    "X_train = X[common_cols]\n",
    "X_test = test_df[common_cols]\n",
    "X_score = score_df[common_cols]\n",
    "\n",
    "y = train_df['Segment1']\n",
    "y_score = score_df['Segment']\n",
    "\n",
    "### í•™ìŠµ\n",
    "model1 = CatBoostClassifier(verbose=100, random_state=42)\n",
    "model1.fit(X_train, y)\n",
    "\n",
    "# ê²€ì¦ ì„±ëŠ¥ í‰ê°€ (3ë‹¨ê³„ê¹Œì§€ ëë‚œ í›„ y_scoreê³¼ ë¹„êµí•˜ì—¬ ê²€ì¦í‰ê°€)\n",
    "score_df['Segment_pred'] = model1.predict(X_score)\n",
    "\n",
    "# ì˜ˆì¸¡\n",
    "test_df['Segment_pred'] = model1.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d52dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['Segment_pred'].value_counts()\n",
    "score_df['Segment_pred'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a72e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"âœ… ë‹¨ê³„ 2: Segment == C or D vs Other (E ì œê±°)\") \n",
    "\n",
    "train_df2 = train_df[train_df['Segment'] != 'E'].copy()\n",
    "test_df2 = test_df[test_df['Segment_pred'] == 'other'].copy()\n",
    "score_df2 = score_df[score_df['Segment_pred']== 'other'].copy()\n",
    "\n",
    "# ë¶„ë¥˜ ë³€ê²½\n",
    "train_df2['Segment1'] = train_df2['Segment'].apply(lambda x: x if x in ['C', 'D'] else 'other')\n",
    "\n",
    "exclude_cols = ['ID', 'ê¸°ì¤€ë…„ì›”', 'Segment', 'Segment1']\n",
    "feature_cols = [\n",
    "    col for col in train_df2.columns\n",
    "    if col not in exclude_cols\n",
    "]\n",
    "\n",
    "# í•™ìŠµìš© í”¼ì²˜/íƒ€ê²Ÿ ì •ì˜\n",
    "X2 = train_df2[feature_cols]\n",
    "y2 = train_df2['Segment1']\n",
    "\n",
    "# Xì™€ test_df ì–‘ìª½ì— ëª¨ë‘ ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ ì„ íƒ\n",
    "common_cols = [col for col in X2.columns if col in test_df2.columns]\n",
    "X_train2 = X2[common_cols]\n",
    "X_test2 = test_df2[common_cols]\n",
    "X_score2 = score_df2[common_cols]\n",
    "\n",
    "### í•™ìŠµ\n",
    "# í´ë˜ìŠ¤ë³„ ê°€ì¤‘ì¹˜ ì ìš©\n",
    "classes = np.unique(train_df2['Segment1'])\n",
    "\n",
    "# ê°€ì¤‘ì¹˜ ìë™ ê³„ì‚°\n",
    "#weights = compute_class_weight(class_weight='balanced', classes=classes, y=train_df2['Segment1'])\n",
    "\n",
    "# ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜\n",
    "#class_weights = weights.tolist()\n",
    "#print(dict(zip(classes, class_weights)))\n",
    "\n",
    "model2 = CatBoostClassifier(\n",
    "    verbose=100, \n",
    "    random_state=42#,\n",
    "    #class_weights=class_weights\n",
    ")\n",
    "\n",
    "model2.fit(X_train2, y2)\n",
    "\n",
    "# ê²€ì¦ ì„±ëŠ¥ í‰ê°€ (3ë‹¨ê³„ê¹Œì§€ ëë‚œ í›„ y_origin_valê³¼ ë¹„êµí•˜ì—¬ ê²€ì¦í‰ê°€)\n",
    "score_df2['Segment_pred'] = model2.predict(X_score2).flatten()\n",
    "\n",
    "# ì˜ˆì¸¡\n",
    "test_df2['Segment_pred'] = model2.predict(X_test2).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ce7509",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df2['Segment_pred'].value_counts()\n",
    "score_df2['Segment_pred'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519addd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2ë‹¨ê³„ ê²°ê³¼ score_dfì— ë°˜ì˜\n",
    "score_df.loc[score_df2.index, 'Segment_pred'] = score_df2['Segment_pred']\n",
    "\n",
    "# ì˜ˆì¸¡ ê²°ê³¼ ë³‘í•©\n",
    "# ì¤‘ë³µëœ ID ì œê±°: ë§ˆì§€ë§‰ ê°’ì„ ê¸°ì¤€ìœ¼ë¡œ ìœ ì§€\n",
    "test_df.loc[test_df2.index, 'Segment_pred'] = test_df2['Segment_pred']\n",
    "    \n",
    "test_df['Segment_pred'].value_counts()\n",
    "score_df['Segment_pred'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08136acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"âœ… ë‹¨ê³„ 3: Segment == A or B (C, D, E ì œì™¸)\")\n",
    "train_df3 = train_df[train_df['Segment'].isin(['A', 'B'])].copy()\n",
    "test_df3 = test_df[test_df['Segment_pred'] == 'other'].copy()\n",
    "score_df3 = score_df[score_df['Segment_pred']== 'other'].copy()\n",
    "\n",
    "# ë¶„ë¥˜ ë³€ê²½\n",
    "train_df3['Segment1'] = train_df3['Segment']\n",
    "\n",
    "exclude_cols = ['ID', 'ê¸°ì¤€ë…„ì›”', 'Segment', 'Segment1']\n",
    "feature_cols = [\n",
    "    col for col in train_df3.columns\n",
    "    if col not in exclude_cols\n",
    "]\n",
    "\n",
    "# í•™ìŠµìš© í”¼ì²˜/íƒ€ê²Ÿ ì •ì˜\n",
    "X3 = train_df3[feature_cols]\n",
    "y3 = train_df3['Segment1']\n",
    "\n",
    "# Xì™€ test_df ì–‘ìª½ì— ëª¨ë‘ ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ ì„ íƒ\n",
    "common_cols = [col for col in X3.columns if col in test_df3.columns]\n",
    "X_train3 = X3[common_cols]\n",
    "X_test3 = test_df3[common_cols]\n",
    "X_score3 = score_df3[common_cols]\n",
    "\n",
    "### í•™ìŠµ\n",
    "model3 = CatBoostClassifier(verbose=100, random_state=42)\n",
    "model3.fit(X_train3, y3)\n",
    "\n",
    "# ê²€ì¦ ì„±ëŠ¥ í‰ê°€ (3ë‹¨ê³„ê¹Œì§€ ëë‚œ í›„ y_origin_valê³¼ ë¹„êµí•˜ì—¬ ê²€ì¦í‰ê°€)\n",
    "score_df3['Segment_pred'] = model3.predict(X_score3)\n",
    "\n",
    "# ì˜ˆì¸¡\n",
    "test_df3['Segment_pred'] = model3.predict(X_test3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f4f175",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df3['Segment_pred'].value_counts()\n",
    "score_df3['Segment_pred'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dddfa41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê²€ì¦ ê²°ê³¼ ë³‘í•©\n",
    "score_df.loc[score_df3.index, 'Segment_pred'] = score_df3['Segment_pred']\n",
    "\n",
    "# ì˜ˆì¸¡ ê²°ê³¼ ë³‘í•©\n",
    "test_df.loc[test_df3.index, 'Segment_pred'] = test_df3['Segment_pred']\n",
    "\n",
    "# ìµœì¢… ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥\n",
    "submission = test_df[['ID', 'Segment_pred']].copy()\n",
    "submission.rename(columns={'Segment_pred': 'Segment'}, inplace=True)\n",
    "submission.to_csv(f'../results/{train_file}_model3.csv', index=False)\n",
    "print(f\"{train_file}_model3 ì €ì¥ ì™„ë£Œ!\")\n",
    "\n",
    "# íŒŒì´í”„ë¼ì¸ ì „ì²´ ì„±ëŠ¥ ê²€ì‚¬\n",
    "f1 = f1_score(y_score, score_df[\"Segment_pred\"], average='micro')\n",
    "print(f\"Validation F1 score (micro): {f1:.4f}\")\n",
    "\n",
    "# í˜¼ë™í–‰ë ¬\n",
    "# í˜¼ë™í–‰ë ¬ì„ DataFrameìœ¼ë¡œ ë³´ê¸° ì¢‹ê²Œ ì¶œë ¥\n",
    "labels = ['A', 'B', 'C', 'D', 'E']\n",
    "cm = confusion_matrix(y_score, score_df[\"Segment_pred\"], labels=labels)\n",
    "cm_df = pd.DataFrame(cm, index=[f\"ì‹¤ì œ_{l}\" for l in labels], columns=[f\"ì˜ˆì¸¡_{l}\" for l in labels])\n",
    "\n",
    "print(\"ğŸ“Š Confusion Matrix\")\n",
    "display(cm_df)\n",
    "\n",
    "class_accuracy = {}\n",
    "for i, label in enumerate(labels):\n",
    "    true_positive = cm[i, i]\n",
    "    total_actual = cm[i, :].sum()\n",
    "    acc = true_positive / total_actual if total_actual > 0 else 0\n",
    "    class_accuracy[label] = round(acc, 4)\n",
    "\n",
    "print(\"ğŸ“Š Segmentë³„ ì •í™•ë„:\")\n",
    "for seg, acc in class_accuracy.items():\n",
    "    print(f\"Segment {seg}: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9859d91c",
   "metadata": {},
   "source": [
    "### íŒŒì´í”„ë¼ì¸ ëª¨ë¸ë§ (2ë‹¨ê³„)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad56e1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# local\n",
    "root_path = '../data/open'\n",
    "\n",
    "# colab\n",
    "# root_path = '/content/drive/MyDrive/12á„Œá…© á„‘á…¡á„‹á…µá„‚á…¥á†¯á„‘á…³á„…á…©á„Œá…¦á†¨á„á…³/data'\n",
    "\n",
    "#train_file = '201812_vif_drop_All'\n",
    "#train_file = '201812_corr_drop_All'\n",
    "#train_file = '201812_processed_All'\n",
    "train_file = 'vif_one_segment'\n",
    "origin_df = pd.read_parquet(f'{root_path}/train/{train_file}.parquet')\n",
    "\n",
    "test_file = f'{root_path}/test/201812_processed_All.parquet'\n",
    "test_df = pd.read_parquet(test_file)\n",
    "\n",
    "y_origin = origin_df['Segment']\n",
    "\n",
    "### í•™ìŠµ/ê²€ì¦ ë¶„ë¦¬ (ì¸ë±ìŠ¤ ìœ ì§€/ y_origin í´ë˜ìŠ¤ ë¹„ìœ¨ì— ë§ì¶°ì„œ)\n",
    "train_df, score_df = train_test_split(origin_df, test_size=0.2, random_state=42, stratify=y_origin)\n",
    "\n",
    "print(\"âœ… ë‹¨ê³„ 1: Segment == C,D,E vs Other\")\n",
    "\n",
    "train_df[\"Segment1\"] = train_df[\"Segment\"].apply(lambda x: x if x in ['C', 'D', 'E'] else 'other')\n",
    "\n",
    "exclude_cols = ['ID', 'ê¸°ì¤€ë…„ì›”', 'Segment', 'Segment1']\n",
    "feature_cols = [\n",
    "    col for col in train_df.columns\n",
    "    if col not in exclude_cols\n",
    "]\n",
    "\n",
    "# í•™ìŠµìš© í”¼ì²˜/íƒ€ê²Ÿ ì •ì˜\n",
    "X = train_df[feature_cols]\n",
    "\n",
    "# Xì™€ test_df ì–‘ìª½ì— ëª¨ë‘ ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ ì„ íƒ\n",
    "common_cols = [col for col in X.columns if col in test_df.columns]\n",
    "X_train = X[common_cols]\n",
    "X_test = test_df[common_cols]\n",
    "X_score = score_df[common_cols]\n",
    "\n",
    "y = train_df['Segment1']\n",
    "y_score = score_df['Segment']\n",
    "\n",
    "### í•™ìŠµ\n",
    "'''\n",
    "# í´ë˜ìŠ¤ë³„ ê°€ì¤‘ì¹˜ ì ìš©\n",
    "classes = np.unique(train_df['Segment1'])\n",
    "\n",
    "# ê°€ì¤‘ì¹˜ ìë™ ê³„ì‚°\n",
    "weights = compute_class_weight(class_weight='balanced', classes=classes, y=train_df['Segment1'])\n",
    "\n",
    "# ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜\n",
    "class_weights = weights.tolist()\n",
    "print(dict(zip(classes, class_weights)))\n",
    "'''\n",
    "model1 = CatBoostClassifier(\n",
    "    verbose=100, \n",
    "    random_state=42\n",
    "    #class_weights=class_weights\n",
    ")\n",
    "model1.fit(X_train, y)\n",
    "\n",
    "# ê²€ì¦ ì„±ëŠ¥ í‰ê°€\n",
    "score_df['Segment_pred'] = model1.predict(X_score).flatten()\n",
    "\n",
    "# ì˜ˆì¸¡\n",
    "test_df['Segment_pred'] = model1.predict(X_test).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795686a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['Segment_pred'].value_counts()\n",
    "score_df['Segment_pred'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac2accf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"âœ… ë‹¨ê³„ 2: Segment == A or B \") \n",
    "\n",
    "train_df2 = train_df[train_df['Segment'].isin(['A', 'B'])].copy()\n",
    "test_df2 = test_df[test_df['Segment_pred'] == 'other'].copy()\n",
    "score_df2 = score_df[score_df['Segment_pred']== 'other'].copy()\n",
    "\n",
    "# ë¶„ë¥˜ ë³€ê²½\n",
    "train_df2['Segment1'] = train_df2['Segment']\n",
    "\n",
    "exclude_cols = ['ID', 'ê¸°ì¤€ë…„ì›”', 'Segment', 'Segment1']\n",
    "feature_cols = [\n",
    "    col for col in train_df2.columns\n",
    "    if col not in exclude_cols\n",
    "]\n",
    "\n",
    "# í•™ìŠµìš© í”¼ì²˜/íƒ€ê²Ÿ ì •ì˜\n",
    "X2 = train_df2[feature_cols]\n",
    "y2 = train_df2['Segment1']\n",
    "\n",
    "# Xì™€ test_df ì–‘ìª½ì— ëª¨ë‘ ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ ì„ íƒ\n",
    "common_cols = [col for col in X2.columns if col in test_df2.columns]\n",
    "X_train2 = X2[common_cols]\n",
    "X_test2 = test_df2[common_cols]\n",
    "X_score2 = score_df2[common_cols]\n",
    "\n",
    "### í•™ìŠµ\n",
    "model2 = CatBoostClassifier(verbose=100, random_state=42)\n",
    "model2.fit(X_train2, y2)\n",
    "\n",
    "# ê²€ì¦ ì„±ëŠ¥ í‰ê°€ (3ë‹¨ê³„ê¹Œì§€ ëë‚œ í›„ y_origin_valê³¼ ë¹„êµí•˜ì—¬ ê²€ì¦í‰ê°€)\n",
    "score_df2['Segment_pred'] = model2.predict(X_score2).flatten()\n",
    "\n",
    "# ì˜ˆì¸¡\n",
    "test_df2['Segment_pred'] = model2.predict(X_test2).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78729e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df2['Segment_pred'].value_counts()\n",
    "score_df2['Segment_pred'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d6c3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2ë‹¨ê³„ ê²°ê³¼ score_dfì— ë°˜ì˜\n",
    "score_df.loc[score_df2.index, 'Segment_pred'] = score_df2['Segment_pred']\n",
    "\n",
    "# ì˜ˆì¸¡ ê²°ê³¼ ë³‘í•©\n",
    "test_df.loc[test_df2.index, 'Segment_pred'] = test_df2['Segment_pred']\n",
    "\n",
    "# ìµœì¢… ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥\n",
    "submission = test_df[['ID', 'Segment_pred']].copy()\n",
    "submission.rename(columns={'Segment_pred': 'Segment'}, inplace=True)\n",
    "submission.to_csv(f'../results/{train_file}_model2.csv', index=False)\n",
    "print(f\"{train_file}_model2 ì €ì¥ ì™„ë£Œ!\")\n",
    "\n",
    "#íŒŒì´í”„ë¼ì¸ ì „ì²´ ì„±ëŠ¥ ê²€ì‚¬\n",
    "f1 = f1_score(y_score, score_df[\"Segment_pred\"], average='micro')\n",
    "print(f\"Validation F1 score (micro): {f1:.4f}\")\n",
    "\n",
    "# í˜¼ë™í–‰ë ¬\n",
    "# í˜¼ë™í–‰ë ¬ì„ DataFrameìœ¼ë¡œ ë³´ê¸° ì¢‹ê²Œ ì¶œë ¥\n",
    "labels = ['A', 'B', 'C', 'D', 'E']\n",
    "cm = confusion_matrix(y_score, score_df[\"Segment_pred\"], labels=labels)\n",
    "cm_df = pd.DataFrame(cm, index=[f\"ì‹¤ì œ_{l}\" for l in labels], columns=[f\"ì˜ˆì¸¡_{l}\" for l in labels])\n",
    "\n",
    "print(\"ğŸ“Š Confusion Matrix\")\n",
    "display(cm_df)\n",
    "\n",
    "class_accuracy = {}\n",
    "for i, label in enumerate(labels):\n",
    "    true_positive = cm[i, i]\n",
    "    total_actual = cm[i, :].sum()\n",
    "    acc = true_positive / total_actual if total_actual > 0 else 0\n",
    "    class_accuracy[label] = round(acc, 4)\n",
    "\n",
    "print(\"ğŸ“Š Segmentë³„ ì •í™•ë„:\")\n",
    "for seg, acc in class_accuracy.items():\n",
    "    print(f\"Segment {seg}: {acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
